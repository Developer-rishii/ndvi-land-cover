{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDVI Land Cover Classification with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📂 2. Load Dataset\n",
    "train = pd.read_csv('/kaggle/input/your-dataset-folder/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/your-dataset-folder/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 3. Get NDVI columns\n",
    "ndvi_cols = [col for col in train.columns if '_N' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 4. Interpolate missing NDVI values\n",
    "train[ndvi_cols] = train[ndvi_cols].interpolate(axis=1, limit_direction='both')\n",
    "test[ndvi_cols] = test[ndvi_cols].interpolate(axis=1, limit_direction='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔉 5. Apply Savitzky-Golay filter to denoise\n",
    "def denoise(row):\n",
    "    return pd.Series(savgol_filter(row, 5, 2), index=row.index)\n",
    "\n",
    "train[ndvi_cols] = train[ndvi_cols].apply(denoise, axis=1)\n",
    "test[ndvi_cols] = test[ndvi_cols].apply(denoise, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 6. Feature Engineering\n",
    "def extract_features(row):\n",
    "    values = row.values\n",
    "    first_half = values[:len(values)//2]\n",
    "    second_half = values[len(values)//2:]\n",
    "    diff = np.diff(values)\n",
    "    trend = np.polyfit(range(len(values)), values, 1)[0]\n",
    "    grad = np.gradient(values)\n",
    "    max_val = values.max()\n",
    "    max_time = np.argmax(values)\n",
    "    above_05 = (values > 0.5).sum()\n",
    "    rising_days = (np.diff(values) > 0).sum()\n",
    "    falling_days = (np.diff(values) < 0).sum()\n",
    "\n",
    "    return pd.Series({\n",
    "        'mean': values.mean(),\n",
    "        'std': values.std(),\n",
    "        'max': max_val,\n",
    "        'min': values.min(),\n",
    "        'range': max_val - values.min(),\n",
    "        'trend': trend,\n",
    "        'mean_first': first_half.mean(),\n",
    "        'mean_second': second_half.mean(),\n",
    "        'std_diff': diff.std(),\n",
    "        'max_diff': diff.max(),\n",
    "        'min_diff': diff.min(),\n",
    "        'grad_mean': grad.mean(),\n",
    "        'grad_std': grad.std(),\n",
    "        'grad_max': grad.max(),\n",
    "        'grad_min': grad.min(),\n",
    "        'peak_day': max_time,\n",
    "        'strong_veg_days': above_05,\n",
    "        'days_rising': rising_days,\n",
    "        'days_falling': falling_days\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feats = train[ndvi_cols].apply(extract_features, axis=1)\n",
    "X_test_feats = test[ndvi_cols].apply(extract_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧠 7. Add PCA Features\n",
    "pca = PCA(n_components=5)\n",
    "pca_train = pd.DataFrame(pca.fit_transform(train[ndvi_cols]), columns=[f'pca_{i}' for i in range(5)])\n",
    "pca_test = pd.DataFrame(pca.transform(test[ndvi_cols]), columns=[f'pca_{i}' for i in range(5)])\n",
    "\n",
    "X_train = pd.concat([X_train_feats, pca_train], axis=1)\n",
    "X_test = pd.concat([X_test_feats, pca_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏆 8. Feature Selection\n",
    "selector = SelectKBest(score_func=f_classif, k=20)\n",
    "X_train_selected = selector.fit_transform(X_train, train['class'])\n",
    "X_test_selected = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 9. Encode Target Labels\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🤖 10. Train Model\n",
    "model = LogisticRegression(solver='saga', max_iter=3000, class_weight='balanced')\n",
    "model.fit(X_train_selected, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📈 11. Cross Validation\n",
    "scores = cross_val_score(model, X_train_selected, y_train, cv=5)\n",
    "print(f"Cross-validation accuracy: {scores.mean():.4f}")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📤 12. Predict & Submit\n",
    "preds = model.predict(X_test_selected)\n",
    "pred_labels = le.inverse_transform(preds)\n",
    "\n",
    "submission = pd.DataFrame({ 'ID': test['ID'], 'class': pred_labels })\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print("✅ submission.csv is ready to download")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}